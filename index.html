<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Robots Pre-Train Robots: Manipulation-
    Centric Robotic Representation from Large-
    Scale Robot Dataset</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<!-- <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div>
    </div>

  </div>
</nav> -->


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">üß∏Robots Pre-Train Robots: Manipulation-
            Centric Robotic Representation from Large-
            Scale Robot Dataset</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
                <a href="https://luccachiang.github.io">Guangqi Jiang</a><sup>1&ast;</sup>,
            </span>
            <span class="author-block">
                <a href="https://guangnianyuji.github.io/">Yifei Sun</a><sup>2&ast;</sup>,
            </span>
            <span class="author-block">
                <a href="https://taohuang13.github.io">Tao Huang</a><sup>3&ast;</sup>,
            </span>
            <span class="author-block">
              <a href="https://github.com/xierhill">Huanyu Li</a><sup>3</sup>,
          </span>
            <span class="author-block">
                <a href="https://cheryyunl.github.io">Yongyuan Liang</a><sup>4&dagger;</sup>,
            </span>
            <span class="author-block">
                <a href="http://hxu.rocks">Huazhe Xu</a><sup>5&dagger;</sup>
            </span>
        </div>
        
        <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC San Diego,</span>
            <span class="author-block"><sup>2</sup>Tongji University,</span>
            <span class="author-block"><sup>3</sup>Shanghai Jiaotong University,</span> <br>
            <span class="author-block"><sup>4</sup>University of Maryland,</span>
            <span class="author-block"><sup>5</sup>Tsinghua University</span>
            <br>&ast; Equal contribution.    &dagger; Equal advising.
        </div>
        

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/luccachiang/robots-pretrain-robots"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href=""
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <img src="static/images/hf_icon.svg" />
                    </span>
                    <span>Models & Dataset</span>
                    </a>
                  </span>
                <span class="link-block">
                  <a href=""
                      class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-twitter"></i>
                    </span>
                    <span>Twitter</span>
                    </a>
                  </span>
              <!-- </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>
    <!--/ Abstract. -->

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div align="center">
        <img src="./static/images/overview.png" alt="Image description" width="100%">
      </div>  
      <br> 
      <!-- <h2 class="subtitle has-text-centered"> -->
      <h2 class="subtitle">
        <!-- Given a scene, our approach (VRB) learns  <strong> actionable representations </strong> for robot learning. VRB predicts contact points and a post-contact trajectory learned from <strong> human videos </strong>.  -->
        <strong> MPI </strong> is an <strong>interaction-oriented</strong> representation learning pipeline for robotic manipulation. Diverging from prior arts grounded in (a) Contrastive Learning, (b) Masked Signal Modeling, or (c) Video Prediction using random frames, our proposed approach in (d) instructs the model towards predicting transition frames and detecting manipulated objects with keyframes as input. As such, the model fosters better comprehension of ‚Äúhow-to-interact‚Äù and ‚Äúwhere-to-interact‚Äù. MPI acquires more informative representations during pre-training and achieves evident improvement across downstream tasks.
      </h2>
      
    </div>

  </div>
</section>
    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>    The pre-training of visual representations has enhanced the efficiency of robot learning. 
            Due to the lack of large-scale in-domain robotic datasets, prior works utilize in-the-wild human videos to pre-train robotic visual representation. 
            Despite their promising results, representations from human videos are inevitably subject to distribution shifts and lack the dynamics information crucial for task completion. 
            We first evaluate various pre-trained representations in terms of their correlation to the downstream robotic manipulation tasks (i.e., manipulation centricity). 
            Interestingly, we find that the ''manipulation centricity''' is a strong indicator of success rates when applied to downstream tasks.
            Drawing from these findings, we propose <strong>M</strong>anipulation <strong>C</strong>entric <strong>R</strong>epresentation (<strong>MCR</strong>), a foundation representation learning framework capturing both visual features and the dynamics information such as actions and proprioceptions of manipulation tasks to improve manipulation centricity. 
            Specifically, we pre-train a visual encoder on the DROID robotic dataset and leverage motion-relevant data such as robot proprioceptive states and actions. 
            We introduce a novel contrastive loss that aligns visual observations with the robot's proprioceptive state-action dynamics, combined with an action prediction loss and a time contrastive loss during pre-training.
            Empirical results across four simulation domains with 20 robotic manipulation tasks demonstrate that \ours~outperforms the strongest baseline by 14.8%. Additionally, <strong>MCR</strong> boosts the success rate in three real-world manipulation tasks by 76.9%. </p>

          </div>
      </div>
    </div>
    </div>
  </section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">

        <div class="content">
          <h2 class="title is-3">Manipulation Centricity of Representations</h2>
          <p>
            When using Grad-CAM to analyze the captured features of existing representations, we observe that a representation‚Äôs downstream task performance appears to correlate with its ability to capture manipulation-relevant regions.
          </p> 
          <p>
            To quantify this metric, we compute the similarity between the regions highlighted by Grad-CAM and the ground truth regions corresponding to the end-effector and task-relevant objects generated by SAM2.  
          </p>

        <div class="columns is-centered has-text-centered">
          <div class="column is-three-fifths has-text-centered">
            <img src="./static/images/metric_calc.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."/>
            <p>Measurement of manipulation centricity</p>
          </div>
        </div>

        <div class="content">
          <h3 class="title is-4">Key findings</h3>
            <p>
              A strong correlation between manipulation centricity and downstream task performance exists, with a Pearson correlation coefficient of $R=0.93$.
              </p>
              <div class="columns is-centered has-text-centered">
                <div class="column is-three-fifths has-text-centered">
                  <img src="./static/images/correlation.png"
                       class="interpolation-image"
                       alt="Interpolate start reference image."/>
                  <p>Correlation between Success rate and Manipulation centricity</p>
                </div>
        </div>

        </div>
      </div>
    </div>

</section>


<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website uses the template from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
